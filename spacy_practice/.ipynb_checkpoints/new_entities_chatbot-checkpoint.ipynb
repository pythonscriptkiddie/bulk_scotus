{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#test using BlackstoneNLP\n",
    "\n",
    "#import standard library modules\n",
    "import sys\n",
    "import json\n",
    "from collections import Counter\n",
    "from typing import List, Any\n",
    "\n",
    "#modules from the community\n",
    "import spacy\n",
    "from dataclasses import dataclass #backported this module from 3.7\n",
    "\n",
    "#import lxml\n",
    "\n",
    "from spacy import displacy\n",
    "from blackstone.displacy_palette import ner_displacy_options\n",
    "\n",
    "#blackstone improved citations\n",
    "from blackstone.pipeline.sentence_segmenter import SentenceSegmenter\n",
    "from blackstone.rules import CITATION_PATTERNS\n",
    "\n",
    "import en_core_web_sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nlp = spacy.load('en_blackstone_proto') #the blackstone model\n",
    "nlp2 = en_core_web_sm.load()\n",
    "#load sentence segmenter\n",
    "sentence_segmenter = SentenceSegmenter(nlp.vocab, CITATION_PATTERNS)\n",
    "nlp.add_pipe(sentence_segmenter, before=\"parser\")\n",
    "\n",
    "#nlp=spacy.load('en_core_web_sm') #the default spacy model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#get the text into a variable from the .txt file\n",
    "def text_from_file(filename: str) -> str:\n",
    "    with open(filename, 'r') as in_file:\n",
    "        data = in_file.read()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#NOTE: we still need to load the doc into a model\n",
    "\n",
    "def text_from_json(filename: str, dict_value: str) -> str:\n",
    "    with open(filename, 'r') as in_file:\n",
    "        data=json.load(in_file)\n",
    "        data=data[dict_value]\n",
    "        return data\n",
    "    \n",
    "#NOTE: we don't use this function until later"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "booker_string = text_from_file('booker_train.txt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#let's start the model\n",
    "bx = nlp(booker_string)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<b>In the section above we can see the law's text displayed using displaCy. We have highlighted the cases, provisions, laws, etc in the text.</b>\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#display_casename_citations_filtered(doc)\n",
    "def get_casename_citations_filtered(doc):\n",
    "    #one\n",
    "    '''Takes a spacy doc object and returns a dictionary of cases using the blackstone nlp model\n",
    "    args:\n",
    "        doc: the spacy doc object\n",
    "    returns:\n",
    "        \n",
    "    '''\n",
    "    cases = (i for i in doc.ents if i.label_ == 'CASENAME')\n",
    "    actual_cases=[]\n",
    "    results = {case.text:[item for item in case] for case in cases}\n",
    "    return results\n",
    "\n",
    "def get_actual_cases(case_list: dict) -> List:\n",
    "    '''Takes the cases and removes some of the ones that are not cases\n",
    "    like the ones without a v in them'''\n",
    "    actual_cases = []\n",
    "    for k, v in case_list.items():\n",
    "        for i in v:\n",
    "            if i.text == 'v.' and i.pos_ == 'ADP' and i.dep_ == 'prep':\n",
    "            #if i.pos_ == 'ADP' and i.dep_ == 'prep':\n",
    "                actual_cases.append(k)\n",
    "    return actual_cases\n",
    "\n",
    "def get_cases_from_doc(doc):\n",
    "    return get_actual_cases(get_casename_citations_filtered(doc))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result_one = get_cases_from_doc(bx)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def clean_case_text(case_list: List[str]) -> List[str]:\n",
    "    '''Takes a list of cases and removes the newline characters at the end'''\n",
    "    clean_cases = (str(i) for i in case_list)\n",
    "    clean_cases = (i.rstrip('\\n') for i in case_list)\n",
    "    return clean_cases"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result_one = list(clean_case_text(result_one))\n",
    "print(result_one)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#for ent in bx.ents:\n",
    "#    print(ent.text)\n",
    "displacy.render(bx, style='ent', options=ner_displacy_options)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for ent in bx.ents:\n",
    "    if ent.label_ == 'JUDGE':\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#New entity training practice"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "new_ent =  '''Had it done so, it could not have found the practical difficulty it has mentioned, Blakely, supra, at 307-308, sufficient to justify the severe limits that its approach imposes upon Congress's legislative authority.'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#get the full name of the file\n",
    "\n",
    "model_name_str = nlp.meta['lang'] + '_' + nlp.meta['name'] + '-' + nlp.meta['version']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(model_name_str)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pipeline = [i for i in nlp.meta['pipeline']]\n",
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from spacy import util\n",
    "util.get_package_path('en_blackstone_proto')\n",
    "\n",
    "nlp.meta"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nlp.meta['version']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#model_data_path = '/Users/thomassullivan/data_science/anaconda3/envs/bx_library/lib/python3.6/site-packages/en_blackstone_proto' + '/' + model_name_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default things to import\n",
    "\n",
    "#test using BlackstoneNLP\n",
    "\n",
    "#import standard library modules\n",
    "import sys\n",
    "import json\n",
    "from collections import Counter\n",
    "from typing import List, Any\n",
    "\n",
    "#modules from the community\n",
    "import spacy\n",
    "from dataclasses import dataclass #backported this module from 3.7\n",
    "\n",
    "#import lxml\n",
    "\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "\n",
    "\n",
    "from blackstone.displacy_palette import ner_displacy_options\n",
    "\n",
    "#blackstone improved citations\n",
    "from blackstone.pipeline.sentence_segmenter import SentenceSegmenter\n",
    "from blackstone.rules import CITATION_PATTERNS\n",
    "\n",
    "import en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_blackstone_proto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_segmenter = SentenceSegmenter(nlp.vocab, CITATION_PATTERNS)\n",
    "\n",
    "ruler = EntityRuler(nlp)\n",
    "#patterns = [{\"label\": \"LEGISLATURE\", \"pattern\": \"Congress\"}]\n",
    "patterns2 = [{\"label\": \"LEGISLATURE\", \"pattern\": [{\"LOWER\": \"congress\"}], \"id\": \"LEGISLATURE\"}]\n",
    "            #{\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}]\n",
    "ruler.add_patterns(patterns2)\n",
    "nlp.add_pipe(sentence_segmenter, before=\"parser\")\n",
    "nlp.add_pipe(ruler, before='ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the text into a variable from the .txt file\n",
    "def text_from_file(filename: str) -> str:\n",
    "    with open(filename, 'r') as in_file:\n",
    "        data = in_file.read()\n",
    "        return data\n",
    "\n",
    "quanta=text_from_file(filename='quanta_case_text.txt')\n",
    "#bx=spacy.load(booker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del booker\n",
    "bx = nlp(quanta)\n",
    "doc = nlp2(quanta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chunk(doc):\n",
    "    chunk=''\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.dep_ == 'dobj':\n",
    "            print('dobj' == token.text)\n",
    "            shift = len([w for w in token.children])\n",
    "            print([w for w in token.children])\n",
    "            chunk=doc[i-shift:i+1]\n",
    "            print(chunk)\n",
    "    return chunk\n",
    "\n",
    "def determine_question_type(chunk):\n",
    "    question_type = 'yesno'\n",
    "    for token in chunk:\n",
    "        if token.dep_ == 'amod':\n",
    "            question_type = 'info'\n",
    "    return question_type\n",
    "\n",
    "def generate_question(doc, question_type):\n",
    "    sent=''\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.tag_ == 'PRP' and doc[i+1].tag_ == 'VBP':\n",
    "            sent = 'do ' + doc[i+1].text\n",
    "            sent = sent + ' ' + doc[i+1:].text\n",
    "            break\n",
    "    doc=nlp(sent)\n",
    "    for i, token in enumerate(bx):\n",
    "        if token.tag_ == 'PRP' and token.text == 'I':\n",
    "            sent = doc[:i].text + ' you ' + doc[i+1].text\n",
    "            break\n",
    "    doc=nlp(sent)\n",
    "    if question_type == 'info':\n",
    "        for i, token in enumerate(doc):\n",
    "            if token.dep_ == 'dobj':\n",
    "                sent = 'why ' + doc[:i].text + ' one ' + doc[i+1:].text\n",
    "                break\n",
    "    doc=nlp(sent)\n",
    "    sent=doc[0].text.capitalize() + ' ' + doc[1:len(doc)-1].text + '?'\n",
    "    return sent\n",
    "\n",
    "#for ent in bx.ents:\n",
    "#    print(ent.text, ent.label_)\n",
    "#dir(spacy.lang)\n",
    "\n",
    "#booker_string = text_from_file('booker_train.txt')\n",
    "#import os\n",
    "#os.chdir(model_data_path)\n",
    "#os.listdir()\n",
    "#test_string = \"\"\"There was before us no dispute as to the relevant statutory scheme or the law as the judge had to apply it. There was no dispute but that the judge had to consider in particular the circumstances in which the evidence came to be made (see section 114(2)(d)), the reliability of the witness Wilson (section 114(2)(e)) and how reliable the making of the statement appears to be (section 114(2)(f)). There was no dispute between the parties that the judge was bound to apply section 114(2) in considering the propriety of reading the transcripts pursuant to section 116 (see R v Cole & Ors [2008] 1 Cr App R No 5, paragraph 6, 7 and 21). Quite apart from those specific provisions the ultimate consideration had to be and remains the fairness of allowing that course to be adopted as Pitchford LJ said in R v Ibrahim [2010] EWCA Crim 1176\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sents = [i for i in bx.sents]\n",
    "def check_sentence(sents):\n",
    "    if len(sents) > 1:\n",
    "        sent = sents[0]\n",
    "        nlp=spacy.load('en_blackstone_proto')\n",
    "        doc=nlp(sent)\n",
    "        chunk=find_chunk(doc)\n",
    "        if str(chunk) == '':\n",
    "            print('The sentence does not contain a direct object.')\n",
    "            return\n",
    "        question_type = determine_question_type(chunk)\n",
    "        question= generate_question(doc, question_type)\n",
    "        print(question)\n",
    "    else:\n",
    "        print('You did not submit a sentence!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [str(i) for i in bx.sents]\n",
    "sents = sents[25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence does not contain a direct object.\n"
     ]
    }
   ],
   "source": [
    "#print(sents[1])\n",
    "#for token in sents[1]:\n",
    "    #print(token.text, token.dep_, token.pos_)\n",
    "check_sentence(sents=sents[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in sents[:10]:\n",
    "#    print(i)\n",
    "   # print(determine_question_type(i))\n",
    "    #print(generate_question(doc=nlp, sent=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_casename_citations_filtered(doc)\n",
    "def get_casename_citations_filtered(doc):\n",
    "    #on\n",
    "    '''Takes a spacy doc object and returns a dictionary of cases using the blackstone nlp model\n",
    "    args:\n",
    "        doc: the spacy doc object\n",
    "    returns:\n",
    "        \n",
    "    '''\n",
    "    cases = (i for i in doc.ents if i.label_ == 'CASENAME')\n",
    "    actual_cases=[]\n",
    "    results = {case.text:[item for item in case] for case in cases}\n",
    "    return results\n",
    "\n",
    "def get_actual_cases(case_list: dict) -> List:\n",
    "    '''Takes the cases and removes some of the ones that are not cases\n",
    "    like the ones without a v in them'''\n",
    "    actual_cases = []\n",
    "    for k, v in case_list.items():\n",
    "        for i in v:\n",
    "            if i.text == 'v.' and i.pos_ == 'ADP' and i.dep_ == 'prep':\n",
    "            #if i.pos_ == 'ADP' and i.dep_ == 'prep':\n",
    "                actual_cases.append(k)\n",
    "    return actual_cases\n",
    "\n",
    "def get_cases_from_doc(doc):\n",
    "    return get_actual_cases(get_casename_citations_filtered(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_one = get_cases_from_doc(bx)\n",
    "#result_one = list(clean_case_text(result_one))\n",
    "#print(result_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "section 114(2)(d) PROVISION\n",
      "section 114(2)(e) PROVISION\n",
      "section 114(2)(f) PROVISION\n",
      "section 114(2) PROVISION\n",
      "section 116 PROVISION\n",
      "R v Cole & Ors CASENAME\n",
      "[2008] 1 Cr App R No 5 CITATION\n",
      "Pitchford LJ JUDGE\n",
      "R v Ibrahim CASENAME\n",
      "[2010] EWCA Crim 1176 CITATION\n"
     ]
    }
   ],
   "source": [
    "for ent in bx.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
