{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test using BlackstoneNLP\n",
    "\n",
    "#import standard library modules\n",
    "import sys\n",
    "import json\n",
    "from collections import Counter\n",
    "from typing import List, Any\n",
    "\n",
    "#modules from the community\n",
    "import spacy\n",
    "from dataclasses import dataclass #backported this module from 3.7\n",
    "\n",
    "#import lxml\n",
    "\n",
    "from spacy import displacy\n",
    "from blackstone.displacy_palette import ner_displacy_options\n",
    "\n",
    "#blackstone improved citations\n",
    "from blackstone.pipeline.sentence_segmenter import SentenceSegmenter\n",
    "from blackstone.rules import CITATION_PATTERNS\n",
    "\n",
    "import en_core_web_sm\n",
    "\n",
    "\n",
    "#BeautifulSoup modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_blackstone_proto') #the blackstone model\n",
    "nlp2 = en_core_web_sm.load()\n",
    "#load sentence segmenter\n",
    "sentence_segmenter = SentenceSegmenter(nlp.vocab, CITATION_PATTERNS)\n",
    "nlp.add_pipe(sentence_segmenter, before=\"parser\")\n",
    "\n",
    "#nlp=spacy.load('en_core_web_sm') #the default spacy model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Let's disable these functions for now\n",
    "\n",
    "def doc_from_file(filename: str, model: Any):\n",
    "    with open(filename, 'r') as in_file:\n",
    "        data = in_file.read()\n",
    "        doc = model(data)\n",
    "        return doc\n",
    "    \n",
    "def doc_from_json(filename: str, model: Any, dict_value: str) -> dict:\n",
    "    with open(filename, 'r') as in_file:\n",
    "        data=json.load(in_file)\n",
    "        doc = model(data[dict_value])\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: we still need to load the doc into a model\n",
    "\n",
    "def text_from_json(filename: str, dict_value: str) -> str:\n",
    "    with open(filename, 'r') as in_file:\n",
    "        data=json.load(in_file)\n",
    "        data=data[dict_value]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfib_v_sebelius = text_from_json(filename='nfib_v_sebelius.json', dict_value='plain_text')\n",
    "\n",
    "#bx = nlp(nfib_data)\n",
    "#lay_doc = nlp2(nfib_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nfib_v_sebelius = nfib_v_sebelius.split('\\n')\n",
    "#nfib_v_sebelius = [i.lstrip() for i in nfib_v_sebelius]\n",
    "\n",
    "def make_clean_text_one(text_file):\n",
    "    text_file = text_file.split('\\n')\n",
    "    text_file = [i.lstrip() for i in text_file]\n",
    "    #for line in text_file:\n",
    "    #    if line[0].isnumeric() == True:\n",
    "    #            print(line)\n",
    "    #            text_file.pop(line)\n",
    "    text_file = ' '.join(text_file)\n",
    "    return text_file\n",
    "\n",
    "def save_clean_text(text_file, filename):\n",
    "    '''Save a clean text file after we have cleaned the data'''\n",
    "    with open(filename, 'w') as out_file:\n",
    "        out_file.write(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfib_v_sebelius = make_clean_text_one(nfib_v_sebelius)\n",
    "\n",
    "save_clean_text(text_file=nfib_v_sebelius, filename='nfib_cleaned.txt')\n",
    "\n",
    "#let's load the models\n",
    "bx = nlp(nfib_v_sebelius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#print(nfib_v_sebelius)\n",
    "\n",
    "#when we have x and y, we create a new example of something to train the model\n",
    "#and we feed it into the model\n",
    "\n",
    "#DISPLAY ENTITIES\n",
    "#for ent in bx.ents:\n",
    "#    print(ent.text, ent.label_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>In the section above we can see the law's text displayed using displaCy. We have highlighted the cases, provisions, laws, etc in the text.</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_casename_citations_filtered(doc)\n",
    "def get_casename_citations_filtered(doc):\n",
    "    #one\n",
    "    '''Takes a spacy doc object and returns a dictionary of cases using the blackstone nlp model\n",
    "    args:\n",
    "        doc: the spacy doc object\n",
    "    returns:\n",
    "        \n",
    "    '''\n",
    "    cases = (i for i in doc.ents if i.label_ == 'CASENAME')\n",
    "    actual_cases=[]\n",
    "    results = {case.text:[item for item in case] for case in cases}\n",
    "    return results\n",
    "\n",
    "def get_actual_cases(case_list: dict) -> List:\n",
    "    '''Takes the cases and removes some of the ones that are not cases\n",
    "    like the ones without a v in them'''\n",
    "    actual_cases = []\n",
    "    for k, v in case_list.items():\n",
    "        for i in v:\n",
    "            if i.text == 'v.' and i.pos_ == 'ADP' and i.dep_ == 'prep':\n",
    "            #if i.pos_ == 'ADP' and i.dep_ == 'prep':\n",
    "                actual_cases.append(k)\n",
    "    return actual_cases\n",
    "\n",
    "def get_cases_from_doc(doc):\n",
    "    return get_actual_cases(get_casename_citations_filtered(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_one = get_cases_from_doc(bx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_case_text(case_list: List[str]) -> List[str]:\n",
    "    '''Takes a list of cases and removes the newline characters at the end'''\n",
    "    clean_cases = (str(i) for i in case_list)\n",
    "    clean_cases = (i.rstrip('\\n') for i in case_list)\n",
    "    return clean_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['United States v. Detroit Timber & Lumber Co.',\n",
       " 'BUSINESS v. SEBELIUS',\n",
       " 'United States v. Lopez',\n",
       " 'United States v. Comstock',\n",
       " 'Hooper v. California',\n",
       " 'Crowell v. Benson',\n",
       " 'Bailey v. Drexel Furniture Co.',\n",
       " 'New York v. United States,',\n",
       " 'Ayotte v. Planned Parenthood of Northern New Eng.,',\n",
       " 'Gibbons v. Ogden',\n",
       " 'Bond v. United States,',\n",
       " 'Wickard v. Filburn',\n",
       " 'Perez v. United States,',\n",
       " 'College Savings Bank v. Florida Prepaid Postsecond- ary Ed.',\n",
       " 'South Dakota v. Dole',\n",
       " 'United States v. Harris',\n",
       " 'Thomas More Law Center v. Obama',\n",
       " 'Enochs v. Williams Packing & Nav.',\n",
       " 'Russello v. United States,',\n",
       " 'Bailey v. George',\n",
       " 'NLRB v. Jones & Laughlin Steel Corp.,',\n",
       " 'Cherokee Nation v. Southern Kansas R. Co.',\n",
       " 'South Carolina v. United States,',\n",
       " 'Heart of Atlanta Motel, Inc. v. United States,',\n",
       " 'Katzenbach v. McClung',\n",
       " 'Kinsella v. United States ex rel.',\n",
       " 'Printz v. United States,',\n",
       " 'Sabri v. United States,',\n",
       " 'Jinks v. Richland County,',\n",
       " 'Parsons v. Bedford',\n",
       " 'New York v. United States',\n",
       " 'Nelson v. Sears, Roebuck & Co.',\n",
       " 'United States v. Reorganized CF&I Fabricators of Utah',\n",
       " 'United States v. Sanchez',\n",
       " 'Sonzinsky v. United States,',\n",
       " 'Springer v. United States,',\n",
       " 'Hylton v. United States',\n",
       " 'Eisner v. Macom-',\n",
       " 'United States v. Butler',\n",
       " 'Barnes v. Gorman',\n",
       " 'Pennhurst State School and Hospital v. Halderman',\n",
       " 'Steward Machine Co. v. Davis',\n",
       " 'Massachusetts v. Mellon',\n",
       " 'FERC v. Mississippi',\n",
       " 'United States v. Booker',\n",
       " 'Hammer v. Dagenhart',\n",
       " 'Helvering v. Davis',\n",
       " 'EEOC v. Wyoming',\n",
       " 'North American Co. v. SEC',\n",
       " 'American Power & Light Co. v. SEC',\n",
       " 'Hodel v. Virginia Surface Mining & Reclamation Assn',\n",
       " 'United States v. Wrightwood Dairy Co.',\n",
       " 'Troxel v. Granville',\n",
       " 'McDonald v. Chicago',\n",
       " 'Albright v. Oliver',\n",
       " 'Hoke v. United States,',\n",
       " 'Perez v. United States, O. T.',\n",
       " 'Nortz v. United States,',\n",
       " 'Schweiker v. Gray Panthers',\n",
       " 'College Savings Bank v. Florida Prepaid Postsecondary Ed',\n",
       " 'Frew v. Hawkins',\n",
       " 'Bennett v. Kentucky Dept.',\n",
       " 'Bowen v. Public Agencies Opposed to Social Security Entrapment,',\n",
       " 'Inc. v. Thornton',\n",
       " 'Baker v. Carr',\n",
       " 'Andrus v. Allard',\n",
       " 'Commodity Futures Trading Commâ€™n v. Schor',\n",
       " 'Scales v. United States,',\n",
       " 'Lipke v. Lederer',\n",
       " 'Staples v. United States,',\n",
       " 'Fullilove v. Klutznick',\n",
       " 'College Savings Bank v. Florida Prepaid Postsecondary Ed.',\n",
       " 'Davis v. Monroe County Bd.',\n",
       " 'Massachusetts v. United States,',\n",
       " 'Metropolitan Washington Airports Authority v. Citizens for Abatement of Aircraft Noise',\n",
       " 'Minne- sota v. Mille Lacs Band of Chippewa Indians,']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we've gotten the cases, let's view the document in displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#judges = [i for i in get_judges(bx, doc)]\n",
    "#for i in judges:\n",
    "#    for j in i:\n",
    "#        print(i.text, [j.text for j in i], end='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_statutes(case_list: List):\n",
    "    '''removes statutes, each of which have a numeric elemnt'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to get the cases\n",
    "#get_actual_cases(get_casename_citations_filtered(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__mod__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'capitalize',\n",
       " 'casefold',\n",
       " 'center',\n",
       " 'count',\n",
       " 'encode',\n",
       " 'endswith',\n",
       " 'expandtabs',\n",
       " 'find',\n",
       " 'format',\n",
       " 'format_map',\n",
       " 'index',\n",
       " 'isalnum',\n",
       " 'isalpha',\n",
       " 'isdecimal',\n",
       " 'isdigit',\n",
       " 'isidentifier',\n",
       " 'islower',\n",
       " 'isnumeric',\n",
       " 'isprintable',\n",
       " 'isspace',\n",
       " 'istitle',\n",
       " 'isupper',\n",
       " 'join',\n",
       " 'ljust',\n",
       " 'lower',\n",
       " 'lstrip',\n",
       " 'maketrans',\n",
       " 'partition',\n",
       " 'replace',\n",
       " 'rfind',\n",
       " 'rindex',\n",
       " 'rjust',\n",
       " 'rpartition',\n",
       " 'rsplit',\n",
       " 'rstrip',\n",
       " 'split',\n",
       " 'splitlines',\n",
       " 'startswith',\n",
       " 'strip',\n",
       " 'swapcase',\n",
       " 'title',\n",
       " 'translate',\n",
       " 'upper',\n",
       " 'zfill']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a = ''\n",
    "#dir(a.ljust(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_found = get_cases_from_doc(doc)\n",
    "print('Cases found: ', [(i, i.label_) for i in cases_found])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dir(spacy.tokens.Token))\n",
    "#for i in cases_found:\n",
    "#    for token in i:\n",
    "#        print(token.text, token.pos_, token.dep_, [i for i in token.lefts], [i for i in token.rights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CaseWithNLP:\n",
    "    case_name: str\n",
    "    name_token: spacy.tokens.Token\n",
    "    plaintiff: spacy.tokens.Token\n",
    "    respondant: spacy.tokens.Token\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_case_list(doc):\n",
    "    result = get_cases_from_doc(doc)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[United States v. Standard Oil Co., Dalehite v. United States,]\n"
     ]
    }
   ],
   "source": [
    "res = make_case_with_nlp(doc)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
